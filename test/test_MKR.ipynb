{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress, IntProgress\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/layers.py\n",
    "import tensorflow as tf\n",
    "from abc import abstractmethod\n",
    "\n",
    "LAYER_IDS = {}\n",
    "\n",
    "\n",
    "def get_layer_id(layer_name=''):\n",
    "    if layer_name not in LAYER_IDS:\n",
    "        LAYER_IDS[layer_name] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        LAYER_IDS[layer_name] += 1\n",
    "        return LAYER_IDS[layer_name]\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, name):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_id(layer))\n",
    "        self.name = name\n",
    "        self.vars = []\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        outputs = self._call(inputs)\n",
    "        return outputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call(self, inputs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, act=tf.nn.relu, name=None):\n",
    "        super(Dense, self).__init__(name)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weight = tf.get_variable(name='weight', shape=(input_dim, output_dim), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name='bias', shape=output_dim, initializer=tf.zeros_initializer())\n",
    "        self.vars = [self.weight]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        output = tf.matmul(x, self.weight) + self.bias\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class CrossCompressUnit(Layer):\n",
    "    def __init__(self, dim, name=None):\n",
    "        super(CrossCompressUnit, self).__init__(name)\n",
    "        self.dim = dim\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weight_vv = tf.get_variable(name='weight_vv', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ev = tf.get_variable(name='weight_ev', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ve = tf.get_variable(name='weight_ve', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ee = tf.get_variable(name='weight_ee', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.bias_v = tf.get_variable(name='bias_v', shape=dim, initializer=tf.zeros_initializer())\n",
    "            self.bias_e = tf.get_variable(name='bias_e', shape=dim, initializer=tf.zeros_initializer())\n",
    "        self.vars = [self.weight_vv, self.weight_ev, self.weight_ve, self.weight_ee]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        # [batch_size, dim]\n",
    "        v, e = inputs\n",
    "\n",
    "        # [batch_size, dim, 1], [batch_size, 1, dim]\n",
    "        v = tf.expand_dims(v, dim=2)\n",
    "        e = tf.expand_dims(e, dim=1)\n",
    "\n",
    "        # [batch_size, dim, dim]\n",
    "        c_matrix = tf.matmul(v, e)\n",
    "        c_matrix_transpose = tf.transpose(c_matrix, perm=[0, 2, 1])\n",
    "\n",
    "        # [batch_size * dim, dim]\n",
    "        c_matrix = tf.reshape(c_matrix, [-1, self.dim])\n",
    "        c_matrix_transpose = tf.reshape(c_matrix_transpose, [-1, self.dim])\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        v_output = tf.reshape(tf.matmul(c_matrix, self.weight_vv) + tf.matmul(c_matrix_transpose, self.weight_ev),\n",
    "                              [-1, self.dim]) + self.bias_v\n",
    "        e_output = tf.reshape(tf.matmul(c_matrix, self.weight_ve) + tf.matmul(c_matrix_transpose, self.weight_ee),\n",
    "                              [-1, self.dim]) + self.bias_e\n",
    "\n",
    "        return v_output, e_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class MKR(object):\n",
    "    def __init__(self, args, n_users, n_items, n_entities, n_relations):\n",
    "        self._parse_args(n_users, n_items, n_entities, n_relations)\n",
    "        self._build_inputs()\n",
    "        self._build_model(args)\n",
    "        self._build_loss(args)\n",
    "        self._build_train(args)\n",
    "\n",
    "    def _parse_args(self, n_users, n_items, n_entities, n_relations):\n",
    "        self.n_user = n_users\n",
    "        self.n_item = n_items\n",
    "        self.n_entity = n_entities\n",
    "        self.n_relation = n_relations\n",
    "\n",
    "        # for computing l2 loss\n",
    "        self.vars_rs = []\n",
    "        self.vars_kge = []\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        self.user_indices = tf.placeholder(tf.int32, [None], 'user_indices')\n",
    "        self.item_indices = tf.placeholder(tf.int32, [None], 'item_indices')\n",
    "        self.labels = tf.placeholder(tf.float32, [None], 'labels')\n",
    "        self.head_indices = tf.placeholder(tf.int32, [None], 'head_indices')\n",
    "        self.tail_indices = tf.placeholder(tf.int32, [None], 'tail_indices')\n",
    "        self.relation_indices = tf.placeholder(tf.int32, [None], 'relation_indices')\n",
    "\n",
    "    def _build_model(self, args):\n",
    "        self._build_low_layers(args)\n",
    "        self._build_high_layers(args)\n",
    "\n",
    "    def _build_low_layers(self, args):\n",
    "        self.user_emb_matrix = tf.get_variable('user_emb_matrix', [self.n_user, args.dim])\n",
    "        self.item_emb_matrix = tf.get_variable('item_emb_matrix', [self.n_item, args.dim])\n",
    "        self.entity_emb_matrix = tf.get_variable('entity_emb_matrix', [self.n_entity, args.dim])\n",
    "        self.relation_emb_matrix = tf.get_variable('relation_emb_matrix', [self.n_relation, args.dim])\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)\n",
    "        self.item_embeddings = tf.nn.embedding_lookup(self.item_emb_matrix, self.item_indices)\n",
    "        self.head_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.head_indices)\n",
    "        self.relation_embeddings = tf.nn.embedding_lookup(self.relation_emb_matrix, self.relation_indices)\n",
    "        self.tail_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.tail_indices)\n",
    "\n",
    "        for _ in range(args.L):\n",
    "            user_mlp = Dense(input_dim=args.dim, output_dim=args.dim)\n",
    "            tail_mlp = Dense(input_dim=args.dim, output_dim=args.dim)\n",
    "            cc_unit = CrossCompressUnit(args.dim)\n",
    "            self.user_embeddings = user_mlp(self.user_embeddings)\n",
    "            self.item_embeddings, self.head_embeddings = cc_unit([self.item_embeddings, self.head_embeddings])\n",
    "            self.tail_embeddings = tail_mlp(self.tail_embeddings)\n",
    "\n",
    "            self.vars_rs.extend(user_mlp.vars)\n",
    "            self.vars_rs.extend(cc_unit.vars)\n",
    "            self.vars_kge.extend(tail_mlp.vars)\n",
    "            self.vars_kge.extend(cc_unit.vars)\n",
    "\n",
    "    def _build_high_layers(self, args):\n",
    "        # RS\n",
    "        use_inner_product = True\n",
    "        if use_inner_product:\n",
    "            # [batch_size]\n",
    "            self.scores = tf.reduce_sum(self.user_embeddings * self.item_embeddings, axis=1)\n",
    "        else:\n",
    "            # [batch_size, dim * 2]\n",
    "            self.user_item_concat = tf.concat([self.user_embeddings, self.item_embeddings], axis=1)\n",
    "            for _ in range(args.H - 1):\n",
    "                rs_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim * 2)\n",
    "                # [batch_size, dim * 2]\n",
    "                self.user_item_concat = rs_mlp(self.user_item_concat)\n",
    "                self.vars_rs.extend(rs_mlp.vars)\n",
    "\n",
    "            rs_pred_mlp = Dense(input_dim=args.dim * 2, output_dim=1)\n",
    "            # [batch_size]\n",
    "            self.scores = tf.squeeze(rs_pred_mlp(self.user_item_concat))\n",
    "            self.vars_rs.extend(rs_pred_mlp.vars)\n",
    "        self.scores_normalized = tf.nn.sigmoid(self.scores)\n",
    "\n",
    "        # KGE\n",
    "        # [batch_size, dim * 2]\n",
    "        self.head_relation_concat = tf.concat([self.head_embeddings, self.relation_embeddings], axis=1)\n",
    "        for _ in range(args.H - 1):\n",
    "            kge_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim * 2)\n",
    "            # [batch_size, dim]\n",
    "            self.head_relation_concat = kge_mlp(self.head_relation_concat)\n",
    "            self.vars_kge.extend(kge_mlp.vars)\n",
    "\n",
    "        kge_pred_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim)\n",
    "        # [batch_size, 1]\n",
    "        self.tail_pred = kge_pred_mlp(self.head_relation_concat)\n",
    "        self.vars_kge.extend(kge_pred_mlp.vars)\n",
    "        self.tail_pred = tf.nn.sigmoid(self.tail_pred)\n",
    "\n",
    "        self.scores_kge = tf.nn.sigmoid(tf.reduce_sum(self.tail_embeddings * self.tail_pred, axis=1))\n",
    "        self.rmse = tf.reduce_mean(\n",
    "            tf.sqrt(tf.reduce_sum(tf.square(self.tail_embeddings - self.tail_pred), axis=1) / args.dim))\n",
    "\n",
    "    def _build_loss(self, args):\n",
    "        # RS\n",
    "        self.base_loss_rs = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))\n",
    "        self.l2_loss_rs = tf.nn.l2_loss(self.user_embeddings) + tf.nn.l2_loss(self.item_embeddings)\n",
    "        for var in self.vars_rs:\n",
    "            self.l2_loss_rs += tf.nn.l2_loss(var)\n",
    "        self.loss_rs = self.base_loss_rs + self.l2_loss_rs * args.l2_weight\n",
    "\n",
    "        # KGE\n",
    "        self.base_loss_kge = -self.scores_kge\n",
    "        self.l2_loss_kge = tf.nn.l2_loss(self.head_embeddings) + tf.nn.l2_loss(self.tail_embeddings)\n",
    "        for var in self.vars_kge:\n",
    "            self.l2_loss_kge += tf.nn.l2_loss(var)\n",
    "        self.loss_kge = self.base_loss_kge + self.l2_loss_kge * args.l2_weight\n",
    "\n",
    "    def _build_train(self, args):\n",
    "        self.optimizer_rs = tf.train.AdamOptimizer(args.lr_rs).minimize(self.loss_rs)\n",
    "        self.optimizer_kge = tf.train.AdamOptimizer(args.lr_kge).minimize(self.loss_kge)\n",
    "\n",
    "    def train_rs(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer_rs, self.loss_rs], feed_dict)\n",
    "\n",
    "    def train_kge(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer_kge, self.rmse], feed_dict)\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "        return auc, acc\n",
    "\n",
    "    def get_scores(self, sess, feed_dict):\n",
    "        return sess.run([self.item_indices, self.scores_normalized], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/train.py\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = str(datetime.timestamp(datetime.now()))\n",
    "\n",
    "# logger = Logger()\n",
    "# session_log_path = \"../log/{}/\".format(timestamp)\n",
    "# logger.create_session_folder(session_log_path)\n",
    "# logger.set_default_filename(session_log_path + \"log.txt\")\n",
    "\n",
    "\n",
    "def train(args, data, show_loss, show_topk):\n",
    "    logger.log(str(args))\n",
    "    n_user, n_item, n_entity, n_relation = data[0], data[1], data[2], data[3]\n",
    "    train_data, eval_data, test_data = data[4], data[5], data[6]\n",
    "    kg = data[7]\n",
    "\n",
    "    n_item = n_item\n",
    "\n",
    "    model = MKR(args, n_user, n_item, n_entity, n_relation)\n",
    "\n",
    "    print(\"n_user : \" , n_user, \"\\n\")\n",
    "    print(\"n_item : \" , n_item, \"\\n\")\n",
    "\n",
    "    # top-K evaluation settings\n",
    "    user_num = 100\n",
    "    k_list = [1, 2, 5, 10, 20, 50, 100]\n",
    "    train_record = get_user_record(train_data, True)\n",
    "    test_record = get_user_record(test_data, False)\n",
    "    user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
    "    if len(user_list) > user_num:\n",
    "        user_list = np.random.choice(user_list, size=user_num, replace=False)\n",
    "    \n",
    "    # item_set = set(list(range(n_item)))\n",
    "    item_set = set()\n",
    "\n",
    "    for data in train_data :\n",
    "        item_set.add(int(data[1]))\n",
    "\n",
    "    for data in eval_data :\n",
    "        item_set.add(int(data[1]))\n",
    "\n",
    "    for data in test_data :\n",
    "        item_set.add(int(data[1]))\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "        for step in range(args.n_epochs):\n",
    "            # RS training\n",
    "            np.random.shuffle(train_data)\n",
    "            start = 0\n",
    "            while start < train_data.shape[0]:\n",
    "                _, loss = model.train_rs(sess, get_feed_dict_for_rs(model, train_data, start, start + args.batch_size))\n",
    "                start += args.batch_size\n",
    "                if show_loss:\n",
    "                    print(loss)\n",
    "\n",
    "            # KGE training\n",
    "            if step % args.kge_interval == 0:\n",
    "                np.random.shuffle(kg)\n",
    "                start = 0\n",
    "                while start < kg.shape[0]:\n",
    "                    _, rmse = model.train_kge(sess, get_feed_dict_for_kge(model, kg, start, start + args.batch_size))\n",
    "                    start += args.batch_size\n",
    "                    if show_loss:\n",
    "                        print(rmse)\n",
    "\n",
    "            # CTR evaluation\n",
    "            train_auc, train_acc = model.eval(sess, get_feed_dict_for_rs(model, train_data, 0, train_data.shape[0]))\n",
    "            eval_auc, eval_acc = model.eval(sess, get_feed_dict_for_rs(model, eval_data, 0, eval_data.shape[0]))\n",
    "            test_auc, test_acc = model.eval(sess, get_feed_dict_for_rs(model, test_data, 0, test_data.shape[0]))\n",
    "\n",
    "            print('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "                  % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "            logger.log('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "                  % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "\n",
    "            # top-K evaluation\n",
    "            if show_topk:\n",
    "                precision, recall, f1 = topk_eval(\n",
    "                    sess, model, user_list, train_record, test_record, item_set, k_list)\n",
    "                print('precision: ', end='')\n",
    "                logger.log('precision: ')\n",
    "                for i in precision:\n",
    "                    print('%.4f\\t' % i, end='')\n",
    "                    logger.log('%.4f\\t' % i)\n",
    "                print()\n",
    "                print('recall: ', end='')\n",
    "                logger.log('recall: ')\n",
    "                for i in recall:\n",
    "                    print('%.4f\\t' % i, end='')\n",
    "                    logger.log('%.4f\\t' % i)\n",
    "                print()\n",
    "                print('f1: ', end='')\n",
    "                logger.log('f1: ')\n",
    "                for i in f1:\n",
    "                    print('%.4f\\t' % i, end='')\n",
    "                    logger.log('%.4f\\t' % i)\n",
    "                print('\\n')\n",
    "            \n",
    "            saver.save(sess, session_log_path + \"models/epoch_{}\".format(step))\n",
    "\n",
    "\n",
    "def get_feed_dict_for_rs(model, data, start, end):\n",
    "    feed_dict = {model.user_indices: data[start:end, 0],\n",
    "                 model.item_indices: data[start:end, 1],\n",
    "                 model.labels: data[start:end, 2],\n",
    "                 model.head_indices: data[start:end, 1]}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def get_feed_dict_for_kge(model, kg, start, end):\n",
    "    feed_dict = {model.item_indices: kg[start:end, 0],\n",
    "                 model.head_indices: kg[start:end, 0],\n",
    "                 model.relation_indices: kg[start:end, 1],\n",
    "                 model.tail_indices: kg[start:end, 2]}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def topk_eval(sess, model, user_list, train_record, test_record, item_set, k_list):\n",
    "    precision_list = {k: [] for k in k_list}\n",
    "    recall_list = {k: [] for k in k_list}\n",
    "\n",
    "    for user in user_list:\n",
    "        test_item_list = list(item_set - train_record[user])\n",
    "        item_score_map = dict()\n",
    "        items, scores = model.get_scores(sess, {model.user_indices: [user] * len(test_item_list),\n",
    "                                                model.item_indices: test_item_list,\n",
    "                                                model.head_indices: test_item_list})\n",
    "        for item, score in zip(items, scores):\n",
    "            item_score_map[item] = score\n",
    "        item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "\n",
    "        for k in k_list:\n",
    "            hit_num = len(set(item_sorted[:k]) & test_record[user])\n",
    "            precision_list[k].append(hit_num / k)\n",
    "            recall_list[k].append(hit_num / len(test_record[user]))\n",
    "\n",
    "    precision = [np.mean(precision_list[k]) for k in k_list]\n",
    "    recall = [np.mean(recall_list[k]) for k in k_list]\n",
    "    f1 = [2 / (1 / precision[i] + 1 / recall[i]) for i in range(len(k_list))]\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def get_user_record(data, is_train):\n",
    "    user_history_dict = dict()\n",
    "    for interaction in data:\n",
    "        user = interaction[0]\n",
    "        item = interaction[1]\n",
    "        label = interaction[2]\n",
    "        if is_train or label == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = set()\n",
    "            user_history_dict[user].add(item)\n",
    "    return user_history_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/data_loader.py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    n_user, n_item, train_data, eval_data, test_data = load_rating(args)\n",
    "    n_entity, n_relation, kg = load_kg(args)\n",
    "    print('data loaded.')\n",
    "\n",
    "    return n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg\n",
    "\n",
    "\n",
    "def load_rating(args):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    # reading rating file\n",
    "    #rating_file = '../data/' + args.dataset + '/ratings_final'\n",
    "    rating_file = '../data/' + 'intersect-14m' + '/ratings_final'\n",
    "    if os.path.exists(rating_file + '.npy'):\n",
    "        rating_np = np.load(rating_file + '.npy')\n",
    "    else:\n",
    "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n",
    "        np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "    n_user = max(set(rating_np[:, 0])) + 1\n",
    "    n_item = max(set(rating_np[:, 1])) + 1\n",
    "    train_data, eval_data, test_data = dataset_split(rating_np)\n",
    "\n",
    "    return n_user, n_item, train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "def dataset_split(rating_np):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    # train:eval:test = 6:2:2\n",
    "    eval_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "    n_ratings = rating_np.shape[0]\n",
    "\n",
    "    eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n",
    "    left = set(range(n_ratings)) - set(eval_indices)\n",
    "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "\n",
    "    train_data = rating_np[train_indices]\n",
    "    eval_data = rating_np[eval_indices]\n",
    "    test_data = rating_np[test_indices]\n",
    "\n",
    "    return train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "def load_kg(args):\n",
    "    print('reading KG file ...')\n",
    "\n",
    "    # reading kg file\n",
    "    #kg_file = '../data/' + args.dataset + '/kg_final'\n",
    "    kg_file = '../data/' + 'intersect-14m' + '/kg_final'\n",
    "    if os.path.exists(kg_file + '.npy'):\n",
    "        kg = np.load(kg_file + '.npy')\n",
    "    else:\n",
    "        kg = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
    "        np.save(kg_file + '.npy', kg)\n",
    "\n",
    "    n_entity = max(set(kg[:, 0]) | set(kg[:, 2]))+1\n",
    "    n_relation = max(set(kg[:, 1]))+1\n",
    "\n",
    "    return n_entity, n_relation, kg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Args Object to Run Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataset = 'movie'\n",
    "        self.n_epoch = 20\n",
    "        self.dim = 8\n",
    "        self.L = 1\n",
    "        self.H = 1\n",
    "        self.batch_size = 4095\n",
    "        self.l2_weight = 1e-6\n",
    "        self.lr_rs = 0.000125\n",
    "        self.lr_kge = 0.000125\n",
    "        self.kge_interval = 3\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "data_info = load_data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separate the preprocesssing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = data_info[0]\n",
    "n_item = data_info[1]\n",
    "n_entity = data_info[2]\n",
    "n_relation = data_info[3]\n",
    "train_data = data_info[4]\n",
    "eval_data = data_info[5]\n",
    "test_data = data_info[6]\n",
    "kg = data_info[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15527"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define path to choose which model and epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"1563359800.887134\"\n",
    "CHOSEN_EPOCH = 19\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 08:15:16.480361 140126775260992 deprecation.py:506] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0729 08:15:16.596993 140126775260992 deprecation.py:506] From <ipython-input-2-a1149e15351b>:47: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0729 08:15:16.601031 140126775260992 deprecation.py:506] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "W0729 08:15:16.650187 140126775260992 deprecation.py:323] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = MKR(args, n_user, n_item, n_entity, n_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 08:15:18.116761 140126775260992 deprecation.py:323] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\n2 root error(s) found.\n  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[node save/Assign_63 (defined at <ipython-input-12-91c373de4425>:2) ]]\n\t [[save/RestoreV2/_36]]\n  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[node save/Assign_63 (defined at <ipython-input-12-91c373de4425>:2) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/Assign_63:\n user_emb_matrix/Adam_1 (defined at <ipython-input-3-b42cb5c6bbd5>:120)\n\nInput Source operations connected to node save/Assign_63:\n user_emb_matrix/Adam_1 (defined at <ipython-input-3-b42cb5c6bbd5>:120)\n\nOriginal stack trace for 'save/Assign_63':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 378, in dispatch_queue\n    yield self.process_one()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 225, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 714, in __init__\n    self.run()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-91c373de4425>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 72, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[{{node save/Assign_63}}]]\n\t [[save/RestoreV2/_36]]\n  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[{{node save/Assign_63}}]]\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1286\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[node save/Assign_63 (defined at <ipython-input-12-91c373de4425>:2) ]]\n\t [[save/RestoreV2/_36]]\n  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[node save/Assign_63 (defined at <ipython-input-12-91c373de4425>:2) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/Assign_63:\n user_emb_matrix/Adam_1 (defined at <ipython-input-3-b42cb5c6bbd5>:120)\n\nInput Source operations connected to node save/Assign_63:\n user_emb_matrix/Adam_1 (defined at <ipython-input-3-b42cb5c6bbd5>:120)\n\nOriginal stack trace for 'save/Assign_63':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 378, in dispatch_queue\n    yield self.process_one()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 225, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 714, in __init__\n    self.run()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-91c373de4425>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 72, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-91c373de4425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1320\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1322\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\n2 root error(s) found.\n  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[node save/Assign_63 (defined at <ipython-input-12-91c373de4425>:2) ]]\n\t [[save/RestoreV2/_36]]\n  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [138493,8] rhs shape= [6036,8]\n\t [[node save/Assign_63 (defined at <ipython-input-12-91c373de4425>:2) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/Assign_63:\n user_emb_matrix/Adam_1 (defined at <ipython-input-3-b42cb5c6bbd5>:120)\n\nInput Source operations connected to node save/Assign_63:\n user_emb_matrix/Adam_1 (defined at <ipython-input-3-b42cb5c6bbd5>:120)\n\nOriginal stack trace for 'save/Assign_63':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 378, in dispatch_queue\n    yield self.process_one()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 225, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 714, in __init__\n    self.run()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-91c373de4425>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 72, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n",
    "saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create positive data dictionary for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_dict = {}\n",
    "for rating in tqdm(train_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)\n",
    "        \n",
    "for rating in tqdm(test_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)\n",
    "        \n",
    "for rating in tqdm(eval_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the frequency of user who liked n movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ns = []\n",
    "for key in truth_dict:\n",
    "    n = len(truth_dict[key])\n",
    "    ns.append(n)\n",
    "\n",
    "ns = Counter(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nscum = {}\n",
    "last = 0\n",
    "for k in sorted(ns):\n",
    "    nscum[k] = ns[k] + last\n",
    "    last = nscum[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cummulative plot\n",
    "plt.figure(figsize=(20,14))\n",
    "plt.plot(list(nscum.keys())[:50], list(nscum.values())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create set of user and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set(truth_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_set = set()\n",
    "for user in tqdm(user_set) :\n",
    "    for movie in truth_dict[user] :\n",
    "        movie_set.add(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create n sample of user to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "user_sample_500 = random.sample(user_set,500)\n",
    "user_sample_1000 = random.sample(user_set,1000)\n",
    "user_sample_3000 = random.sample(user_set,3000)\n",
    "user_sample_5000 = random.sample(user_set,5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define function to predict k recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_suggestion(user,k) : \n",
    "    item_score_map = dict()\n",
    "    items, scores = model.get_scores(sess, {model.user_indices: [user] * len(movie_set),\n",
    "                                            model.item_indices: list(movie_set),\n",
    "                                            model.head_indices: list(movie_set)})\n",
    "    for item, score in zip(items, scores):\n",
    "            item_score_map[item] = score\n",
    "\n",
    "    item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "    \n",
    "    return item_score_pair_sorted[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_truth(user, k):\n",
    "    if user not in truth_dict:\n",
    "        return []\n",
    "    #ERASE [:k]\n",
    "    return truth_dict[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define function to evaluate  prec@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_pred_truth(pred, truth, k):\n",
    "    pred_item_set = {x[0] for x in pred}\n",
    "    truth_item_set = set(truth)\n",
    "    \n",
    "    return pred_item_set.intersection(truth_item_set)\n",
    "\n",
    "def check_precision_at_k(sample_user, k):\n",
    "    \n",
    "    pred = get_top_suggestion(sample_user, k)\n",
    "    truth = get_top_truth(sample_user, k)\n",
    "    \n",
    "    intersect = get_intersect_pred_truth(pred, truth, k)\n",
    "    \n",
    "    if len(truth) > 0 :\n",
    "        return intersect, len(intersect) / len(truth)\n",
    "    else:\n",
    "        return {}, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the prec@k for every n user sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_500):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_1000):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3000 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_3000):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5000 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_5000):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Distinct Rate of every n sample user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_500\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in range(1, 10):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_1000\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in tqdm(range(1, 10)):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_3000\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in tqdm(range(1, 10)):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_5000\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in tqdm(range(1, 10)):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Unique Movies recommended on n user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_500) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_1000) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_3000) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_5000) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate top k recommendation for 10 selected user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initiate 10 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_list = [20169,66966,82374,4296,10204,123623,115870,128970,83750,97239]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict 10 recommendation for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_dict = {}\n",
    "\n",
    "for user in user_test_list :\n",
    "    pred_tuple = get_top_suggestion(user-1,10)\n",
    "    pred = [x[0] for x in pred_tuple]\n",
    "    topk_pred_dict[user] = pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Movie ID to Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = open(\"../data/intersect-14m/moviesIdx2.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entities = {}\n",
    "for movie in movies:\n",
    "    x = movie.strip().split()\n",
    "    movie_id = int(x[0])\n",
    "    movie_name = x[1]\n",
    "    dict_entities[movie_id] = movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_movie(id_movie_list, truth_list) :\n",
    "    res = []\n",
    "    for id_movie in id_movie_list :\n",
    "        is_watched = \"watched\" if id_movie in truth_list else \"nope\"\n",
    "        content = dict_entities[int(id_movie)] + \" > \" + is_watched \n",
    "        res.append(content)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_dict_title = {}\n",
    "\n",
    "for user in topk_pred_dict.keys() :\n",
    "    topk_pred_dict_title[user] = get_list_movie(topk_pred_dict[user],truth_dict[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topk_pred_df = pd.DataFrame(topk_pred_dict_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_path =  \"../log/{}/topk_pred.csv\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_df.to_csv(topk_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
