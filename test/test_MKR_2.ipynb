{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress, IntProgress\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/layers.py\n",
    "import tensorflow as tf\n",
    "from abc import abstractmethod\n",
    "\n",
    "LAYER_IDS = {}\n",
    "\n",
    "\n",
    "def get_layer_id(layer_name=''):\n",
    "    if layer_name not in LAYER_IDS:\n",
    "        LAYER_IDS[layer_name] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        LAYER_IDS[layer_name] += 1\n",
    "        return LAYER_IDS[layer_name]\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, name):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_id(layer))\n",
    "        self.name = name\n",
    "        self.vars = []\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        outputs = self._call(inputs)\n",
    "        return outputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call(self, inputs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, act=tf.nn.relu, name=None):\n",
    "        super(Dense, self).__init__(name)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weight = tf.get_variable(name='weight', shape=(input_dim, output_dim), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name='bias', shape=output_dim, initializer=tf.zeros_initializer())\n",
    "        self.vars = [self.weight]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        output = tf.matmul(x, self.weight) + self.bias\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class CrossCompressUnit(Layer):\n",
    "    def __init__(self, dim, name=None):\n",
    "        super(CrossCompressUnit, self).__init__(name)\n",
    "        self.dim = dim\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weight_vv = tf.get_variable(name='weight_vv', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ev = tf.get_variable(name='weight_ev', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ve = tf.get_variable(name='weight_ve', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ee = tf.get_variable(name='weight_ee', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.bias_v = tf.get_variable(name='bias_v', shape=dim, initializer=tf.zeros_initializer())\n",
    "            self.bias_e = tf.get_variable(name='bias_e', shape=dim, initializer=tf.zeros_initializer())\n",
    "        self.vars = [self.weight_vv, self.weight_ev, self.weight_ve, self.weight_ee]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        # [batch_size, dim]\n",
    "        v, e = inputs\n",
    "\n",
    "        # [batch_size, dim, 1], [batch_size, 1, dim]\n",
    "        v = tf.expand_dims(v, dim=2)\n",
    "        e = tf.expand_dims(e, dim=1)\n",
    "\n",
    "        # [batch_size, dim, dim]\n",
    "        c_matrix = tf.matmul(v, e)\n",
    "        c_matrix_transpose = tf.transpose(c_matrix, perm=[0, 2, 1])\n",
    "\n",
    "        # [batch_size * dim, dim]\n",
    "        c_matrix = tf.reshape(c_matrix, [-1, self.dim])\n",
    "        c_matrix_transpose = tf.reshape(c_matrix_transpose, [-1, self.dim])\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        v_output = tf.reshape(tf.matmul(c_matrix, self.weight_vv) + tf.matmul(c_matrix_transpose, self.weight_ev),\n",
    "                              [-1, self.dim]) + self.bias_v\n",
    "        e_output = tf.reshape(tf.matmul(c_matrix, self.weight_ve) + tf.matmul(c_matrix_transpose, self.weight_ee),\n",
    "                              [-1, self.dim]) + self.bias_e\n",
    "\n",
    "        return v_output, e_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class MKR(object):\n",
    "    def __init__(self, args, n_users, n_items, n_entities, n_relations):\n",
    "        self._parse_args(n_users, n_items, n_entities, n_relations)\n",
    "        self._build_inputs()\n",
    "        self._build_model(args)\n",
    "        self._build_loss(args)\n",
    "        self._build_train(args)\n",
    "\n",
    "    def _parse_args(self, n_users, n_items, n_entities, n_relations):\n",
    "        self.n_user = n_users\n",
    "        self.n_item = n_items\n",
    "        self.n_entity = n_entities\n",
    "        self.n_relation = n_relations\n",
    "\n",
    "        # for computing l2 loss\n",
    "        self.vars_rs = []\n",
    "        self.vars_kge = []\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        self.user_indices = tf.placeholder(tf.int32, [None], 'user_indices')\n",
    "        self.item_indices = tf.placeholder(tf.int32, [None], 'item_indices')\n",
    "        self.labels = tf.placeholder(tf.float32, [None], 'labels')\n",
    "        self.head_indices = tf.placeholder(tf.int32, [None], 'head_indices')\n",
    "        self.tail_indices = tf.placeholder(tf.int32, [None], 'tail_indices')\n",
    "        self.relation_indices = tf.placeholder(tf.int32, [None], 'relation_indices')\n",
    "\n",
    "    def _build_model(self, args):\n",
    "        self._build_low_layers(args)\n",
    "        self._build_high_layers(args)\n",
    "\n",
    "    def _build_low_layers(self, args):\n",
    "        self.user_emb_matrix = tf.get_variable('user_emb_matrix', [self.n_user, args.dim])\n",
    "        self.item_emb_matrix = tf.get_variable('item_emb_matrix', [self.n_item, args.dim])\n",
    "        self.entity_emb_matrix = tf.get_variable('entity_emb_matrix', [self.n_entity, args.dim])\n",
    "        self.relation_emb_matrix = tf.get_variable('relation_emb_matrix', [self.n_relation, args.dim])\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)\n",
    "        self.item_embeddings = tf.nn.embedding_lookup(self.item_emb_matrix, self.item_indices)\n",
    "        self.head_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.head_indices)\n",
    "        self.relation_embeddings = tf.nn.embedding_lookup(self.relation_emb_matrix, self.relation_indices)\n",
    "        self.tail_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.tail_indices)\n",
    "\n",
    "        for _ in range(args.L):\n",
    "            user_mlp = Dense(input_dim=args.dim, output_dim=args.dim)\n",
    "            tail_mlp = Dense(input_dim=args.dim, output_dim=args.dim)\n",
    "            cc_unit = CrossCompressUnit(args.dim)\n",
    "            self.user_embeddings = user_mlp(self.user_embeddings)\n",
    "            self.item_embeddings, self.head_embeddings = cc_unit([self.item_embeddings, self.head_embeddings])\n",
    "            self.tail_embeddings = tail_mlp(self.tail_embeddings)\n",
    "\n",
    "            self.vars_rs.extend(user_mlp.vars)\n",
    "            self.vars_rs.extend(cc_unit.vars)\n",
    "            self.vars_kge.extend(tail_mlp.vars)\n",
    "            self.vars_kge.extend(cc_unit.vars)\n",
    "\n",
    "    def _build_high_layers(self, args):\n",
    "        # RS\n",
    "        use_inner_product = True\n",
    "        if use_inner_product:\n",
    "            # [batch_size]\n",
    "            self.scores = tf.reduce_sum(self.user_embeddings * self.item_embeddings, axis=1)\n",
    "        else:\n",
    "            # [batch_size, dim * 2]\n",
    "            self.user_item_concat = tf.concat([self.user_embeddings, self.item_embeddings], axis=1)\n",
    "            for _ in range(args.H - 1):\n",
    "                rs_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim * 2)\n",
    "                # [batch_size, dim * 2]\n",
    "                self.user_item_concat = rs_mlp(self.user_item_concat)\n",
    "                self.vars_rs.extend(rs_mlp.vars)\n",
    "\n",
    "            rs_pred_mlp = Dense(input_dim=args.dim * 2, output_dim=1)\n",
    "            # [batch_size]\n",
    "            self.scores = tf.squeeze(rs_pred_mlp(self.user_item_concat))\n",
    "            self.vars_rs.extend(rs_pred_mlp.vars)\n",
    "        self.scores_normalized = tf.nn.sigmoid(self.scores)\n",
    "\n",
    "        # KGE\n",
    "        # [batch_size, dim * 2]\n",
    "        self.head_relation_concat = tf.concat([self.head_embeddings, self.relation_embeddings], axis=1)\n",
    "        for _ in range(args.H - 1):\n",
    "            kge_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim * 2)\n",
    "            # [batch_size, dim]\n",
    "            self.head_relation_concat = kge_mlp(self.head_relation_concat)\n",
    "            self.vars_kge.extend(kge_mlp.vars)\n",
    "\n",
    "        kge_pred_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim)\n",
    "        # [batch_size, 1]\n",
    "        self.tail_pred = kge_pred_mlp(self.head_relation_concat)\n",
    "        self.vars_kge.extend(kge_pred_mlp.vars)\n",
    "        self.tail_pred = tf.nn.sigmoid(self.tail_pred)\n",
    "\n",
    "        self.scores_kge = tf.nn.sigmoid(tf.reduce_sum(self.tail_embeddings * self.tail_pred, axis=1))\n",
    "        self.rmse = tf.reduce_mean(\n",
    "            tf.sqrt(tf.reduce_sum(tf.square(self.tail_embeddings - self.tail_pred), axis=1) / args.dim))\n",
    "\n",
    "    def _build_loss(self, args):\n",
    "        # RS\n",
    "        self.base_loss_rs = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))\n",
    "        self.l2_loss_rs = tf.nn.l2_loss(self.user_embeddings) + tf.nn.l2_loss(self.item_embeddings)\n",
    "        for var in self.vars_rs:\n",
    "            self.l2_loss_rs += tf.nn.l2_loss(var)\n",
    "        self.loss_rs = self.base_loss_rs + self.l2_loss_rs * args.l2_weight\n",
    "\n",
    "        # KGE\n",
    "        self.base_loss_kge = -self.scores_kge\n",
    "        self.l2_loss_kge = tf.nn.l2_loss(self.head_embeddings) + tf.nn.l2_loss(self.tail_embeddings)\n",
    "        for var in self.vars_kge:\n",
    "            self.l2_loss_kge += tf.nn.l2_loss(var)\n",
    "        self.loss_kge = self.base_loss_kge + self.l2_loss_kge * args.l2_weight\n",
    "\n",
    "    def _build_train(self, args):\n",
    "        self.optimizer_rs = tf.train.AdamOptimizer(args.lr_rs).minimize(self.loss_rs)\n",
    "        self.optimizer_kge = tf.train.AdamOptimizer(args.lr_kge).minimize(self.loss_kge)\n",
    "\n",
    "    def train_rs(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer_rs, self.loss_rs], feed_dict)\n",
    "\n",
    "    def train_kge(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer_kge, self.rmse], feed_dict)\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "        return auc, acc\n",
    "\n",
    "    def get_scores(self, sess, feed_dict):\n",
    "        return sess.run([self.item_indices, self.scores_normalized], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/train.py\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = str(datetime.timestamp(datetime.now()))\n",
    "\n",
    "# logger = Logger()\n",
    "# session_log_path = \"../log/{}/\".format(timestamp)\n",
    "# logger.create_session_folder(session_log_path)\n",
    "# logger.set_default_filename(session_log_path + \"log.txt\")\n",
    "\n",
    "\n",
    "def train(args, data, show_loss, show_topk):\n",
    "    logger.log(str(args))\n",
    "    n_user, n_item, n_entity, n_relation = data[0], data[1], data[2], data[3]\n",
    "    train_data, eval_data, test_data = data[4], data[5], data[6]\n",
    "    kg = data[7]\n",
    "\n",
    "    n_item = n_item\n",
    "\n",
    "    model = MKR(args, n_user, n_item, n_entity, n_relation)\n",
    "\n",
    "    print(\"n_user : \" , n_user, \"\\n\")\n",
    "    print(\"n_item : \" , n_item, \"\\n\")\n",
    "\n",
    "    # top-K evaluation settings\n",
    "    user_num = 100\n",
    "    k_list = [1, 2, 5, 10, 20, 50, 100]\n",
    "    train_record = get_user_record(train_data, True)\n",
    "    test_record = get_user_record(test_data, False)\n",
    "    user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
    "    if len(user_list) > user_num:\n",
    "        user_list = np.random.choice(user_list, size=user_num, replace=False)\n",
    "    \n",
    "    # item_set = set(list(range(n_item)))\n",
    "    item_set = set()\n",
    "\n",
    "    for data in train_data :\n",
    "        item_set.add(int(data[1]))\n",
    "\n",
    "    for data in eval_data :\n",
    "        item_set.add(int(data[1]))\n",
    "\n",
    "    for data in test_data :\n",
    "        item_set.add(int(data[1]))\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "        for step in range(args.n_epochs):\n",
    "            # RS training\n",
    "            np.random.shuffle(train_data)\n",
    "            start = 0\n",
    "            while start < train_data.shape[0]:\n",
    "                _, loss = model.train_rs(sess, get_feed_dict_for_rs(model, train_data, start, start + args.batch_size))\n",
    "                start += args.batch_size\n",
    "                if show_loss:\n",
    "                    print(loss)\n",
    "\n",
    "            # KGE training\n",
    "            if step % args.kge_interval == 0:\n",
    "                np.random.shuffle(kg)\n",
    "                start = 0\n",
    "                while start < kg.shape[0]:\n",
    "                    _, rmse = model.train_kge(sess, get_feed_dict_for_kge(model, kg, start, start + args.batch_size))\n",
    "                    start += args.batch_size\n",
    "                    if show_loss:\n",
    "                        print(rmse)\n",
    "\n",
    "            # CTR evaluation\n",
    "            train_auc, train_acc = model.eval(sess, get_feed_dict_for_rs(model, train_data, 0, train_data.shape[0]))\n",
    "            eval_auc, eval_acc = model.eval(sess, get_feed_dict_for_rs(model, eval_data, 0, eval_data.shape[0]))\n",
    "            test_auc, test_acc = model.eval(sess, get_feed_dict_for_rs(model, test_data, 0, test_data.shape[0]))\n",
    "\n",
    "            print('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "                  % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "            logger.log('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "                  % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "\n",
    "            # top-K evaluation\n",
    "            if show_topk:\n",
    "                precision, recall, f1 = topk_eval(\n",
    "                    sess, model, user_list, train_record, test_record, item_set, k_list)\n",
    "                print('precision: ', end='')\n",
    "                logger.log('precision: ')\n",
    "                for i in precision:\n",
    "                    print('%.4f\\t' % i, end='')\n",
    "                    logger.log('%.4f\\t' % i)\n",
    "                print()\n",
    "                print('recall: ', end='')\n",
    "                logger.log('recall: ')\n",
    "                for i in recall:\n",
    "                    print('%.4f\\t' % i, end='')\n",
    "                    logger.log('%.4f\\t' % i)\n",
    "                print()\n",
    "                print('f1: ', end='')\n",
    "                logger.log('f1: ')\n",
    "                for i in f1:\n",
    "                    print('%.4f\\t' % i, end='')\n",
    "                    logger.log('%.4f\\t' % i)\n",
    "                print('\\n')\n",
    "            \n",
    "            saver.save(sess, session_log_path + \"models/epoch_{}\".format(step))\n",
    "\n",
    "\n",
    "def get_feed_dict_for_rs(model, data, start, end):\n",
    "    feed_dict = {model.user_indices: data[start:end, 0],\n",
    "                 model.item_indices: data[start:end, 1],\n",
    "                 model.labels: data[start:end, 2],\n",
    "                 model.head_indices: data[start:end, 1]}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def get_feed_dict_for_kge(model, kg, start, end):\n",
    "    feed_dict = {model.item_indices: kg[start:end, 0],\n",
    "                 model.head_indices: kg[start:end, 0],\n",
    "                 model.relation_indices: kg[start:end, 1],\n",
    "                 model.tail_indices: kg[start:end, 2]}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def topk_eval(sess, model, user_list, train_record, test_record, item_set, k_list):\n",
    "    precision_list = {k: [] for k in k_list}\n",
    "    recall_list = {k: [] for k in k_list}\n",
    "\n",
    "    for user in user_list:\n",
    "        test_item_list = list(item_set - train_record[user])\n",
    "        item_score_map = dict()\n",
    "        items, scores = model.get_scores(sess, {model.user_indices: [user] * len(test_item_list),\n",
    "                                                model.item_indices: test_item_list,\n",
    "                                                model.head_indices: test_item_list})\n",
    "        for item, score in zip(items, scores):\n",
    "            item_score_map[item] = score\n",
    "        item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "\n",
    "        for k in k_list:\n",
    "            hit_num = len(set(item_sorted[:k]) & test_record[user])\n",
    "            precision_list[k].append(hit_num / k)\n",
    "            recall_list[k].append(hit_num / len(test_record[user]))\n",
    "\n",
    "    precision = [np.mean(precision_list[k]) for k in k_list]\n",
    "    recall = [np.mean(recall_list[k]) for k in k_list]\n",
    "    f1 = [2 / (1 / precision[i] + 1 / recall[i]) for i in range(len(k_list))]\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def get_user_record(data, is_train):\n",
    "    user_history_dict = dict()\n",
    "    for interaction in data:\n",
    "        user = interaction[0]\n",
    "        item = interaction[1]\n",
    "        label = interaction[2]\n",
    "        if is_train or label == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = set()\n",
    "            user_history_dict[user].add(item)\n",
    "    return user_history_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/data_loader.py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    n_user, n_item, train_data, eval_data, test_data = load_rating(args)\n",
    "    n_entity, n_relation, kg = load_kg(args)\n",
    "    print('data loaded.')\n",
    "\n",
    "    return n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg\n",
    "\n",
    "\n",
    "def load_rating(args):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    # reading rating file\n",
    "    #rating_file = '../data/' + args.dataset + '/ratings_final'\n",
    "    rating_file = '../data/' + 'intersect-14m' + '/ratings_final'\n",
    "    if os.path.exists(rating_file + '.npy'):\n",
    "        rating_np = np.load(rating_file + '.npy')\n",
    "    else:\n",
    "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n",
    "        np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "    n_user = max(set(rating_np[:, 0])) + 1\n",
    "    n_item = max(set(rating_np[:, 1])) + 1\n",
    "    train_data, eval_data, test_data = dataset_split(rating_np)\n",
    "\n",
    "    return n_user, n_item, train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "def dataset_split(rating_np):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    # train:eval:test = 6:2:2\n",
    "    eval_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "    n_ratings = rating_np.shape[0]\n",
    "\n",
    "    eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n",
    "    left = set(range(n_ratings)) - set(eval_indices)\n",
    "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "\n",
    "    train_data = rating_np[train_indices]\n",
    "    eval_data = rating_np[eval_indices]\n",
    "    test_data = rating_np[test_indices]\n",
    "\n",
    "    return train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "def load_kg(args):\n",
    "    print('reading KG file ...')\n",
    "\n",
    "    # reading kg file\n",
    "    #kg_file = '../data/' + args.dataset + '/kg_final'\n",
    "    kg_file = '../data/' + 'intersect-14m' + '/kg_final'\n",
    "    if os.path.exists(kg_file + '.npy'):\n",
    "        kg = np.load(kg_file + '.npy')\n",
    "    else:\n",
    "        kg = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
    "        np.save(kg_file + '.npy', kg)\n",
    "\n",
    "    n_entity = max(set(kg[:, 0]) | set(kg[:, 2]))+1\n",
    "    n_relation = max(set(kg[:, 1]))+1\n",
    "\n",
    "    return n_entity, n_relation, kg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Args Object to Run Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataset = 'movie'\n",
    "        self.n_epoch = 20\n",
    "        self.dim = 8\n",
    "        self.L = 1\n",
    "        self.H = 1\n",
    "        self.batch_size = 4095\n",
    "        self.l2_weight = 1e-6\n",
    "        self.lr_rs = 0.000125\n",
    "        self.lr_kge = 0.000125\n",
    "        self.kge_interval = 3\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "data_info = load_data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separate the preprocesssing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = data_info[0]\n",
    "n_item = data_info[1]\n",
    "n_entity = data_info[2]\n",
    "n_relation = data_info[3]\n",
    "train_data = data_info[4]\n",
    "eval_data = data_info[5]\n",
    "test_data = data_info[6]\n",
    "kg = data_info[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138493"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15527"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define path to choose which model and epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"1563340721.225733\"\n",
    "CHOSEN_EPOCH = 19\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 08:25:53.692734 140212434138944 deprecation.py:506] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0729 08:25:53.874209 140212434138944 deprecation.py:506] From <ipython-input-2-a1149e15351b>:47: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0729 08:25:53.883348 140212434138944 deprecation.py:506] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "W0729 08:25:53.956866 140212434138944 deprecation.py:323] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = MKR(args, n_user, n_item, n_entity, n_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 08:25:56.372026 140212434138944 deprecation.py:323] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n",
    "saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create positive data dictionary for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8468027/8468027 [01:01<00:00, 136591.99it/s]\n",
      "100%|██████████| 2822675/2822675 [00:22<00:00, 125520.37it/s]\n",
      "100%|██████████| 2822675/2822675 [00:22<00:00, 123724.11it/s]\n"
     ]
    }
   ],
   "source": [
    "truth_dict = {}\n",
    "for rating in tqdm(train_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)\n",
    "        \n",
    "for rating in tqdm(test_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)\n",
    "        \n",
    "for rating in tqdm(eval_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the frequency of user who liked n movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ns = []\n",
    "for key in truth_dict:\n",
    "    n = len(truth_dict[key])\n",
    "    ns.append(n)\n",
    "\n",
    "ns = Counter(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({51: 780,\n",
       "         28: 1779,\n",
       "         99: 255,\n",
       "         12: 4048,\n",
       "         34: 1318,\n",
       "         10: 4124,\n",
       "         79: 357,\n",
       "         38: 1172,\n",
       "         8: 3781,\n",
       "         15: 3410,\n",
       "         245: 33,\n",
       "         14: 3468,\n",
       "         25: 1876,\n",
       "         88: 295,\n",
       "         13: 3924,\n",
       "         41: 998,\n",
       "         7: 3365,\n",
       "         64: 487,\n",
       "         47: 841,\n",
       "         49: 794,\n",
       "         172: 79,\n",
       "         37: 1249,\n",
       "         19: 2557,\n",
       "         6: 2879,\n",
       "         59: 632,\n",
       "         5: 2293,\n",
       "         104: 229,\n",
       "         21: 2376,\n",
       "         43: 931,\n",
       "         101: 232,\n",
       "         1: 510,\n",
       "         32: 1444,\n",
       "         33: 1385,\n",
       "         20: 2505,\n",
       "         44: 939,\n",
       "         4: 1822,\n",
       "         53: 694,\n",
       "         103: 271,\n",
       "         240: 38,\n",
       "         11: 4054,\n",
       "         350: 14,\n",
       "         35: 1290,\n",
       "         63: 595,\n",
       "         26: 1815,\n",
       "         27: 1832,\n",
       "         31: 1428,\n",
       "         56: 677,\n",
       "         30: 1523,\n",
       "         24: 2043,\n",
       "         72: 401,\n",
       "         39: 1106,\n",
       "         16: 3251,\n",
       "         123: 185,\n",
       "         9: 4189,\n",
       "         145: 119,\n",
       "         111: 182,\n",
       "         18: 2813,\n",
       "         45: 812,\n",
       "         40: 1022,\n",
       "         132: 139,\n",
       "         50: 780,\n",
       "         29: 1614,\n",
       "         73: 406,\n",
       "         2: 866,\n",
       "         126: 147,\n",
       "         46: 852,\n",
       "         80: 353,\n",
       "         114: 167,\n",
       "         119: 176,\n",
       "         120: 195,\n",
       "         165: 87,\n",
       "         135: 145,\n",
       "         78: 411,\n",
       "         3: 1289,\n",
       "         58: 634,\n",
       "         84: 336,\n",
       "         94: 283,\n",
       "         836: 1,\n",
       "         54: 726,\n",
       "         0: 341,\n",
       "         75: 387,\n",
       "         108: 239,\n",
       "         69: 499,\n",
       "         96: 294,\n",
       "         517: 6,\n",
       "         163: 101,\n",
       "         23: 2081,\n",
       "         71: 459,\n",
       "         22: 2186,\n",
       "         105: 224,\n",
       "         137: 131,\n",
       "         220: 47,\n",
       "         193: 75,\n",
       "         70: 504,\n",
       "         248: 33,\n",
       "         139: 124,\n",
       "         122: 160,\n",
       "         86: 302,\n",
       "         98: 259,\n",
       "         125: 176,\n",
       "         85: 323,\n",
       "         238: 32,\n",
       "         164: 87,\n",
       "         76: 386,\n",
       "         95: 247,\n",
       "         204: 58,\n",
       "         121: 163,\n",
       "         216: 36,\n",
       "         355: 9,\n",
       "         499: 5,\n",
       "         131: 141,\n",
       "         256: 35,\n",
       "         258: 16,\n",
       "         65: 520,\n",
       "         199: 59,\n",
       "         564: 4,\n",
       "         81: 404,\n",
       "         62: 539,\n",
       "         67: 532,\n",
       "         87: 337,\n",
       "         217: 44,\n",
       "         17: 2996,\n",
       "         142: 139,\n",
       "         115: 187,\n",
       "         57: 649,\n",
       "         92: 267,\n",
       "         36: 1252,\n",
       "         74: 407,\n",
       "         134: 150,\n",
       "         192: 72,\n",
       "         42: 974,\n",
       "         124: 148,\n",
       "         61: 589,\n",
       "         97: 238,\n",
       "         282: 17,\n",
       "         83: 371,\n",
       "         77: 407,\n",
       "         107: 240,\n",
       "         150: 115,\n",
       "         345: 13,\n",
       "         66: 505,\n",
       "         274: 30,\n",
       "         272: 18,\n",
       "         133: 139,\n",
       "         52: 705,\n",
       "         177: 64,\n",
       "         221: 36,\n",
       "         102: 257,\n",
       "         144: 118,\n",
       "         203: 49,\n",
       "         230: 50,\n",
       "         234: 32,\n",
       "         183: 86,\n",
       "         250: 35,\n",
       "         118: 187,\n",
       "         130: 142,\n",
       "         481: 2,\n",
       "         188: 56,\n",
       "         141: 129,\n",
       "         208: 43,\n",
       "         757: 1,\n",
       "         161: 107,\n",
       "         254: 23,\n",
       "         580: 2,\n",
       "         324: 14,\n",
       "         48: 835,\n",
       "         138: 128,\n",
       "         205: 60,\n",
       "         68: 466,\n",
       "         224: 34,\n",
       "         143: 116,\n",
       "         170: 63,\n",
       "         181: 61,\n",
       "         226: 39,\n",
       "         109: 213,\n",
       "         90: 295,\n",
       "         374: 8,\n",
       "         432: 4,\n",
       "         136: 139,\n",
       "         184: 56,\n",
       "         156: 95,\n",
       "         285: 24,\n",
       "         213: 50,\n",
       "         55: 682,\n",
       "         214: 53,\n",
       "         189: 59,\n",
       "         210: 51,\n",
       "         298: 10,\n",
       "         288: 20,\n",
       "         219: 35,\n",
       "         207: 44,\n",
       "         174: 74,\n",
       "         112: 229,\n",
       "         152: 102,\n",
       "         186: 47,\n",
       "         149: 134,\n",
       "         89: 319,\n",
       "         155: 122,\n",
       "         187: 66,\n",
       "         173: 81,\n",
       "         315: 15,\n",
       "         82: 376,\n",
       "         361: 4,\n",
       "         129: 158,\n",
       "         160: 100,\n",
       "         218: 30,\n",
       "         106: 206,\n",
       "         60: 614,\n",
       "         305: 17,\n",
       "         201: 53,\n",
       "         284: 29,\n",
       "         255: 22,\n",
       "         93: 255,\n",
       "         347: 8,\n",
       "         253: 29,\n",
       "         168: 96,\n",
       "         261: 27,\n",
       "         175: 73,\n",
       "         190: 65,\n",
       "         196: 64,\n",
       "         265: 18,\n",
       "         153: 115,\n",
       "         140: 127,\n",
       "         241: 34,\n",
       "         91: 312,\n",
       "         242: 23,\n",
       "         185: 66,\n",
       "         215: 44,\n",
       "         100: 264,\n",
       "         179: 82,\n",
       "         276: 20,\n",
       "         228: 42,\n",
       "         117: 175,\n",
       "         365: 14,\n",
       "         148: 115,\n",
       "         249: 27,\n",
       "         151: 101,\n",
       "         269: 21,\n",
       "         222: 36,\n",
       "         235: 37,\n",
       "         232: 25,\n",
       "         171: 90,\n",
       "         252: 26,\n",
       "         519: 4,\n",
       "         459: 7,\n",
       "         202: 78,\n",
       "         113: 217,\n",
       "         162: 105,\n",
       "         233: 40,\n",
       "         158: 92,\n",
       "         146: 122,\n",
       "         116: 181,\n",
       "         159: 90,\n",
       "         334: 15,\n",
       "         388: 4,\n",
       "         296: 27,\n",
       "         197: 52,\n",
       "         147: 120,\n",
       "         128: 179,\n",
       "         200: 49,\n",
       "         169: 74,\n",
       "         167: 79,\n",
       "         195: 46,\n",
       "         423: 6,\n",
       "         398: 8,\n",
       "         335: 16,\n",
       "         843: 1,\n",
       "         289: 16,\n",
       "         191: 55,\n",
       "         368: 11,\n",
       "         359: 12,\n",
       "         209: 46,\n",
       "         558: 4,\n",
       "         166: 91,\n",
       "         387: 7,\n",
       "         524: 3,\n",
       "         154: 113,\n",
       "         229: 49,\n",
       "         378: 7,\n",
       "         351: 2,\n",
       "         292: 21,\n",
       "         180: 86,\n",
       "         157: 86,\n",
       "         194: 63,\n",
       "         382: 6,\n",
       "         428: 2,\n",
       "         295: 23,\n",
       "         291: 12,\n",
       "         275: 16,\n",
       "         635: 4,\n",
       "         551: 3,\n",
       "         573: 3,\n",
       "         262: 35,\n",
       "         206: 45,\n",
       "         110: 225,\n",
       "         332: 10,\n",
       "         178: 90,\n",
       "         383: 12,\n",
       "         451: 5,\n",
       "         402: 10,\n",
       "         354: 7,\n",
       "         176: 55,\n",
       "         373: 12,\n",
       "         328: 9,\n",
       "         239: 36,\n",
       "         273: 30,\n",
       "         236: 26,\n",
       "         243: 33,\n",
       "         271: 23,\n",
       "         356: 4,\n",
       "         805: 1,\n",
       "         409: 6,\n",
       "         309: 20,\n",
       "         518: 2,\n",
       "         1290: 1,\n",
       "         441: 5,\n",
       "         333: 13,\n",
       "         227: 44,\n",
       "         929: 1,\n",
       "         376: 6,\n",
       "         442: 8,\n",
       "         557: 3,\n",
       "         244: 36,\n",
       "         537: 2,\n",
       "         358: 8,\n",
       "         302: 17,\n",
       "         430: 8,\n",
       "         297: 16,\n",
       "         336: 12,\n",
       "         247: 32,\n",
       "         327: 14,\n",
       "         223: 41,\n",
       "         264: 29,\n",
       "         438: 8,\n",
       "         277: 28,\n",
       "         316: 11,\n",
       "         394: 9,\n",
       "         344: 16,\n",
       "         343: 12,\n",
       "         303: 25,\n",
       "         211: 34,\n",
       "         268: 35,\n",
       "         414: 6,\n",
       "         362: 16,\n",
       "         318: 7,\n",
       "         393: 9,\n",
       "         389: 8,\n",
       "         329: 14,\n",
       "         266: 30,\n",
       "         653: 4,\n",
       "         320: 23,\n",
       "         370: 17,\n",
       "         127: 170,\n",
       "         501: 5,\n",
       "         478: 6,\n",
       "         246: 22,\n",
       "         198: 69,\n",
       "         530: 5,\n",
       "         225: 43,\n",
       "         424: 7,\n",
       "         474: 5,\n",
       "         845: 1,\n",
       "         182: 77,\n",
       "         689: 4,\n",
       "         364: 13,\n",
       "         367: 12,\n",
       "         260: 37,\n",
       "         541: 3,\n",
       "         514: 4,\n",
       "         380: 6,\n",
       "         421: 6,\n",
       "         372: 7,\n",
       "         525: 3,\n",
       "         366: 16,\n",
       "         251: 28,\n",
       "         308: 17,\n",
       "         281: 19,\n",
       "         287: 13,\n",
       "         257: 23,\n",
       "         212: 42,\n",
       "         357: 17,\n",
       "         304: 17,\n",
       "         1547: 1,\n",
       "         436: 5,\n",
       "         337: 14,\n",
       "         425: 6,\n",
       "         561: 1,\n",
       "         237: 33,\n",
       "         419: 10,\n",
       "         710: 1,\n",
       "         666: 2,\n",
       "         279: 20,\n",
       "         391: 9,\n",
       "         467: 3,\n",
       "         504: 2,\n",
       "         456: 2,\n",
       "         313: 16,\n",
       "         360: 13,\n",
       "         880: 1,\n",
       "         566: 4,\n",
       "         306: 19,\n",
       "         465: 6,\n",
       "         411: 6,\n",
       "         270: 24,\n",
       "         307: 18,\n",
       "         476: 3,\n",
       "         429: 4,\n",
       "         314: 14,\n",
       "         801: 1,\n",
       "         510: 2,\n",
       "         263: 25,\n",
       "         460: 4,\n",
       "         448: 2,\n",
       "         301: 19,\n",
       "         319: 10,\n",
       "         353: 14,\n",
       "         612: 2,\n",
       "         331: 11,\n",
       "         450: 5,\n",
       "         405: 11,\n",
       "         278: 29,\n",
       "         280: 20,\n",
       "         473: 3,\n",
       "         317: 14,\n",
       "         283: 17,\n",
       "         852: 1,\n",
       "         413: 10,\n",
       "         433: 9,\n",
       "         592: 4,\n",
       "         377: 7,\n",
       "         417: 3,\n",
       "         294: 15,\n",
       "         293: 18,\n",
       "         267: 30,\n",
       "         724: 1,\n",
       "         311: 7,\n",
       "         605: 5,\n",
       "         641: 2,\n",
       "         312: 19,\n",
       "         384: 8,\n",
       "         231: 29,\n",
       "         342: 4,\n",
       "         286: 19,\n",
       "         299: 16,\n",
       "         696: 1,\n",
       "         412: 8,\n",
       "         454: 3,\n",
       "         535: 2,\n",
       "         624: 2,\n",
       "         555: 1,\n",
       "         395: 6,\n",
       "         330: 14,\n",
       "         458: 2,\n",
       "         493: 3,\n",
       "         407: 5,\n",
       "         534: 2,\n",
       "         686: 2,\n",
       "         811: 1,\n",
       "         310: 15,\n",
       "         669: 2,\n",
       "         484: 1,\n",
       "         737: 1,\n",
       "         453: 4,\n",
       "         300: 16,\n",
       "         445: 5,\n",
       "         463: 7,\n",
       "         392: 7,\n",
       "         887: 1,\n",
       "         507: 4,\n",
       "         626: 4,\n",
       "         900: 1,\n",
       "         538: 6,\n",
       "         452: 7,\n",
       "         259: 26,\n",
       "         544: 2,\n",
       "         385: 9,\n",
       "         323: 8,\n",
       "         446: 3,\n",
       "         1330: 1,\n",
       "         346: 10,\n",
       "         549: 4,\n",
       "         804: 2,\n",
       "         352: 6,\n",
       "         565: 2,\n",
       "         349: 6,\n",
       "         1071: 1,\n",
       "         820: 1,\n",
       "         390: 3,\n",
       "         1185: 1,\n",
       "         418: 7,\n",
       "         554: 3,\n",
       "         582: 2,\n",
       "         662: 2,\n",
       "         480: 4,\n",
       "         437: 1,\n",
       "         720: 1,\n",
       "         610: 1,\n",
       "         443: 5,\n",
       "         575: 3,\n",
       "         727: 1,\n",
       "         717: 4,\n",
       "         457: 4,\n",
       "         339: 10,\n",
       "         578: 1,\n",
       "         631: 2,\n",
       "         422: 4,\n",
       "         585: 2,\n",
       "         426: 6,\n",
       "         475: 5,\n",
       "         468: 3,\n",
       "         434: 6,\n",
       "         410: 3,\n",
       "         498: 3,\n",
       "         386: 9,\n",
       "         440: 6,\n",
       "         628: 1,\n",
       "         415: 6,\n",
       "         489: 1,\n",
       "         599: 3,\n",
       "         290: 12,\n",
       "         883: 1,\n",
       "         379: 6,\n",
       "         396: 5,\n",
       "         341: 12,\n",
       "         338: 7,\n",
       "         408: 7,\n",
       "         449: 2,\n",
       "         397: 6,\n",
       "         503: 4,\n",
       "         321: 13,\n",
       "         752: 1,\n",
       "         604: 1,\n",
       "         570: 2,\n",
       "         516: 3,\n",
       "         521: 3,\n",
       "         798: 1,\n",
       "         439: 4,\n",
       "         543: 2,\n",
       "         488: 1,\n",
       "         486: 2,\n",
       "         623: 2,\n",
       "         1170: 1,\n",
       "         400: 4,\n",
       "         742: 1,\n",
       "         483: 4,\n",
       "         1295: 1,\n",
       "         529: 1,\n",
       "         369: 8,\n",
       "         416: 6,\n",
       "         401: 5,\n",
       "         477: 3,\n",
       "         506: 3,\n",
       "         677: 2,\n",
       "         533: 3,\n",
       "         651: 2,\n",
       "         522: 5,\n",
       "         659: 2,\n",
       "         888: 1,\n",
       "         593: 1,\n",
       "         746: 1,\n",
       "         399: 9,\n",
       "         322: 9,\n",
       "         587: 2,\n",
       "         678: 1,\n",
       "         528: 3,\n",
       "         371: 7,\n",
       "         571: 2,\n",
       "         988: 1,\n",
       "         1053: 1,\n",
       "         620: 2,\n",
       "         496: 2,\n",
       "         619: 2,\n",
       "         707: 1,\n",
       "         326: 8,\n",
       "         375: 5,\n",
       "         539: 2,\n",
       "         916: 2,\n",
       "         701: 1,\n",
       "         548: 1,\n",
       "         540: 1,\n",
       "         494: 1,\n",
       "         381: 7,\n",
       "         747: 1,\n",
       "         348: 8,\n",
       "         673: 1,\n",
       "         851: 2,\n",
       "         513: 6,\n",
       "         650: 1,\n",
       "         833: 1,\n",
       "         652: 2,\n",
       "         634: 1,\n",
       "         505: 2,\n",
       "         632: 1,\n",
       "         726: 3,\n",
       "         403: 4,\n",
       "         431: 1,\n",
       "         850: 1,\n",
       "         495: 2,\n",
       "         586: 1,\n",
       "         406: 4,\n",
       "         532: 1,\n",
       "         462: 4,\n",
       "         497: 1,\n",
       "         559: 2,\n",
       "         638: 3,\n",
       "         1032: 1,\n",
       "         1132: 1,\n",
       "         325: 7,\n",
       "         542: 1,\n",
       "         1019: 1,\n",
       "         471: 4,\n",
       "         732: 2,\n",
       "         739: 1,\n",
       "         512: 4,\n",
       "         981: 1,\n",
       "         588: 2,\n",
       "         683: 1,\n",
       "         482: 4,\n",
       "         340: 4,\n",
       "         760: 1,\n",
       "         464: 3,\n",
       "         523: 2,\n",
       "         509: 5,\n",
       "         1054: 1,\n",
       "         444: 5,\n",
       "         547: 2,\n",
       "         886: 1,\n",
       "         420: 2,\n",
       "         584: 2,\n",
       "         531: 2,\n",
       "         469: 4,\n",
       "         644: 1,\n",
       "         511: 2,\n",
       "         363: 5,\n",
       "         896: 1,\n",
       "         705: 1,\n",
       "         722: 1,\n",
       "         576: 1,\n",
       "         867: 1,\n",
       "         654: 3,\n",
       "         889: 2,\n",
       "         611: 2,\n",
       "         606: 1,\n",
       "         1156: 1,\n",
       "         687: 2,\n",
       "         630: 1,\n",
       "         600: 2,\n",
       "         568: 1,\n",
       "         716: 2,\n",
       "         404: 5,\n",
       "         645: 2,\n",
       "         560: 2,\n",
       "         1436: 1,\n",
       "         479: 1,\n",
       "         546: 3,\n",
       "         711: 1,\n",
       "         772: 1,\n",
       "         810: 1,\n",
       "         502: 2,\n",
       "         769: 1,\n",
       "         693: 2,\n",
       "         637: 2,\n",
       "         763: 2,\n",
       "         1047: 1,\n",
       "         625: 3,\n",
       "         596: 1,\n",
       "         461: 2,\n",
       "         789: 1,\n",
       "         731: 1,\n",
       "         1755: 1,\n",
       "         447: 3,\n",
       "         562: 1,\n",
       "         671: 1,\n",
       "         765: 2,\n",
       "         556: 1,\n",
       "         741: 1,\n",
       "         730: 1,\n",
       "         738: 1,\n",
       "         1158: 1,\n",
       "         733: 1,\n",
       "         690: 1,\n",
       "         771: 1,\n",
       "         863: 1,\n",
       "         1029: 1,\n",
       "         1203: 1,\n",
       "         629: 1,\n",
       "         761: 2,\n",
       "         427: 5,\n",
       "         700: 1,\n",
       "         697: 1,\n",
       "         713: 1,\n",
       "         485: 2,\n",
       "         472: 3,\n",
       "         508: 3,\n",
       "         734: 1,\n",
       "         736: 1,\n",
       "         688: 1,\n",
       "         595: 1,\n",
       "         706: 1,\n",
       "         613: 1,\n",
       "         960: 1,\n",
       "         775: 1,\n",
       "         500: 2,\n",
       "         795: 1,\n",
       "         1188: 1,\n",
       "         603: 1,\n",
       "         675: 1,\n",
       "         455: 1,\n",
       "         709: 1,\n",
       "         1190: 1,\n",
       "         1014: 1,\n",
       "         621: 1,\n",
       "         545: 1,\n",
       "         723: 1,\n",
       "         785: 1,\n",
       "         567: 2,\n",
       "         1087: 1,\n",
       "         607: 1,\n",
       "         694: 1,\n",
       "         435: 2,\n",
       "         526: 1,\n",
       "         993: 1,\n",
       "         674: 1,\n",
       "         768: 1,\n",
       "         643: 1,\n",
       "         614: 1,\n",
       "         1545: 1,\n",
       "         691: 1,\n",
       "         616: 1,\n",
       "         491: 2,\n",
       "         1283: 1,\n",
       "         1018: 1,\n",
       "         861: 1,\n",
       "         615: 1,\n",
       "         748: 1,\n",
       "         536: 1,\n",
       "         622: 2,\n",
       "         1003: 1,\n",
       "         965: 1,\n",
       "         598: 1,\n",
       "         1118: 1,\n",
       "         776: 1,\n",
       "         2127: 1,\n",
       "         1352: 1,\n",
       "         828: 1,\n",
       "         702: 1,\n",
       "         520: 1,\n",
       "         940: 1,\n",
       "         818: 1,\n",
       "         663: 1,\n",
       "         1512: 1,\n",
       "         618: 1,\n",
       "         487: 2,\n",
       "         581: 2,\n",
       "         670: 1,\n",
       "         1425: 1,\n",
       "         793: 1,\n",
       "         759: 1,\n",
       "         579: 1,\n",
       "         466: 1,\n",
       "         1251: 1,\n",
       "         684: 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nscum = {}\n",
    "last = 0\n",
    "for k in sorted(ns):\n",
    "    nscum[k] = ns[k] + last\n",
    "    last = nscum[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8464433278>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cummulative plot\n",
    "plt.figure(figsize=(20,14))\n",
    "plt.plot(list(nscum.keys())[:50], list(nscum.values())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create set of user and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set(truth_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138493/138493 [00:03<00:00, 40852.18it/s]\n"
     ]
    }
   ],
   "source": [
    "movie_set = set()\n",
    "for user in tqdm(user_set) :\n",
    "    for movie in truth_dict[user] :\n",
    "        movie_set.add(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create n sample of user to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "user_sample_500 = random.sample(user_set,500)\n",
    "user_sample_1000 = random.sample(user_set,1000)\n",
    "user_sample_3000 = random.sample(user_set,3000)\n",
    "user_sample_5000 = random.sample(user_set,5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define function to predict k recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_suggestion(user,k) : \n",
    "    item_score_map = dict()\n",
    "    items, scores = model.get_scores(sess, {model.user_indices: [user] * len(movie_set),\n",
    "                                            model.item_indices: list(movie_set),\n",
    "                                            model.head_indices: list(movie_set)})\n",
    "    for item, score in zip(items, scores):\n",
    "            item_score_map[item] = score\n",
    "\n",
    "    item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "    \n",
    "    return item_score_pair_sorted[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_truth(user, k):\n",
    "    if user not in truth_dict:\n",
    "        return []\n",
    "    #ERASE [:k]\n",
    "    return truth_dict[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define function to evaluate  prec@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_pred_truth(pred, truth, k):\n",
    "    pred_item_set = {x[0] for x in pred}\n",
    "    truth_item_set = set(truth)\n",
    "    \n",
    "    return pred_item_set.intersection(truth_item_set)\n",
    "\n",
    "def check_precision_at_k(sample_user, k):\n",
    "    \n",
    "    pred = get_top_suggestion(sample_user, k)\n",
    "    truth = get_top_truth(sample_user, k)\n",
    "    \n",
    "    intersect = get_intersect_pred_truth(pred, truth, k)\n",
    "    \n",
    "    if len(truth) > 0 :\n",
    "        return intersect, len(intersect) / len(truth)\n",
    "    else:\n",
    "        return {}, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the prec@k for every n user sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:18<00:00, 26.86it/s]\n"
     ]
    }
   ],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_500):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010526315789473683"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.49it/s]\n"
     ]
    }
   ],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_1000):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3000 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:50<00:00, 27.08it/s]\n"
     ]
    }
   ],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_3000):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0101010101010101e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5000 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:03<00:00, 27.40it/s]\n"
     ]
    }
   ],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(user_sample_5000):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.491546188731736e-05"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Distinct Rate of every n sample user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distinct rate\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_500\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in range(1, 10):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 27.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distinct rate\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_1000\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in tqdm(range(1, 10)):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 27.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distinct rate\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_3000\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in tqdm(range(1, 10)):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 27.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distinct rate\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "offset = 190 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = user_sample_5000\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in tqdm(range(1, 10)):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    #print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "#print(\"\\nintersect\")\n",
    "#print(intersect, len(intersect))\n",
    "#print(\"\\nunion\")\n",
    "#print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Unique Movies recommended on n user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:18<00:00, 27.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_500) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_1000) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:30<00:00, 33.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_3000) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5000 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:50<00:00, 45.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_movies = set()\n",
    "\n",
    "for user in tqdm(user_sample_5000) :    \n",
    "    pred = get_top_suggestion(user,10)\n",
    "    for x in pred :\n",
    "        unique_movies.add(x[0])\n",
    "        \n",
    "print(len(unique_movies))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate top k recommendation for 10 selected user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initiate 10 sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_list = [20169,66966,82374,4296,10204,123623,115870,128970,83750,97239]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict 10 recommendation for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_dict = {}\n",
    "\n",
    "for user in user_test_list :\n",
    "    pred_tuple = get_top_suggestion(user-1,10)\n",
    "    pred = [x[0] for x in pred_tuple]\n",
    "    topk_pred_dict[user] = pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20169: [364, 14850, 9305, 6159, 13705, 9147, 1432, 9995, 14524, 6168],\n",
       " 66966: [9305, 9995, 14269, 6168, 8980, 364, 8678, 3671, 1497, 5785],\n",
       " 82374: [364, 9305, 9983, 14524, 3671, 6168, 14850, 9995, 5785, 8980],\n",
       " 4296: [364, 9305, 14524, 14850, 6159, 6168, 7787, 1524, 8951, 11963],\n",
       " 10204: [9305, 1497, 7787, 3844, 10501, 1815, 8678, 2205, 3991, 364],\n",
       " 123623: [9305, 1497, 3844, 7787, 9983, 1815, 364, 3671, 10501, 8678],\n",
       " 115870: [9305, 9995, 8678, 364, 3671, 14269, 6168, 8980, 5785, 1497],\n",
       " 128970: [10118, 14269, 7224, 8678, 11626, 7175, 8084, 4516, 5994, 9934],\n",
       " 83750: [10118, 14269, 8084, 6856, 4516, 9934, 5994, 11626, 12520, 4160],\n",
       " 97239: [9305, 364, 14850, 9995, 6168, 6159, 14524, 5785, 3067, 13471]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Movie ID to Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = open(\"../data/intersect-14m/moviesIdx2.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entities = {}\n",
    "for movie in movies:\n",
    "    x = movie.strip().split()\n",
    "    movie_id = int(x[0])\n",
    "    movie_name = x[1]\n",
    "    dict_entities[movie_id] = movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_movie(id_movie_list, truth_list) :\n",
    "    res = []\n",
    "    for id_movie in id_movie_list :\n",
    "        is_watched = \"watched\" if id_movie in truth_list else \"nope\"\n",
    "        content = dict_entities[int(id_movie)] + \" > \" + is_watched \n",
    "        res.append(content)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_dict_title = {}\n",
    "\n",
    "for user in topk_pred_dict.keys() :\n",
    "    topk_pred_dict_title[user] = get_list_movie(topk_pred_dict[user],truth_dict[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topk_pred_df = pd.DataFrame(topk_pred_dict_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_path =  \"../log/{}/topk_pred.csv\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_pred_df.to_csv(topk_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
