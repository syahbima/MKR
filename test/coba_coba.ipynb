{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/layers.py\n",
    "import tensorflow as tf\n",
    "from abc import abstractmethod\n",
    "\n",
    "LAYER_IDS = {}\n",
    "\n",
    "\n",
    "def get_layer_id(layer_name=''):\n",
    "    if layer_name not in LAYER_IDS:\n",
    "        LAYER_IDS[layer_name] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        LAYER_IDS[layer_name] += 1\n",
    "        return LAYER_IDS[layer_name]\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, name):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_id(layer))\n",
    "        self.name = name\n",
    "        self.vars = []\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        outputs = self._call(inputs)\n",
    "        return outputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call(self, inputs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, act=tf.nn.relu, name=None):\n",
    "        super(Dense, self).__init__(name)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weight = tf.get_variable(name='weight', shape=(input_dim, output_dim), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name='bias', shape=output_dim, initializer=tf.zeros_initializer())\n",
    "        self.vars = [self.weight]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        output = tf.matmul(x, self.weight) + self.bias\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class CrossCompressUnit(Layer):\n",
    "    def __init__(self, dim, name=None):\n",
    "        super(CrossCompressUnit, self).__init__(name)\n",
    "        self.dim = dim\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weight_vv = tf.get_variable(name='weight_vv', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ev = tf.get_variable(name='weight_ev', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ve = tf.get_variable(name='weight_ve', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.weight_ee = tf.get_variable(name='weight_ee', shape=(dim, 1), dtype=tf.float32)\n",
    "            self.bias_v = tf.get_variable(name='bias_v', shape=dim, initializer=tf.zeros_initializer())\n",
    "            self.bias_e = tf.get_variable(name='bias_e', shape=dim, initializer=tf.zeros_initializer())\n",
    "        self.vars = [self.weight_vv, self.weight_ev, self.weight_ve, self.weight_ee]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        # [batch_size, dim]\n",
    "        v, e = inputs\n",
    "\n",
    "        # [batch_size, dim, 1], [batch_size, 1, dim]\n",
    "        v = tf.expand_dims(v, dim=2)\n",
    "        e = tf.expand_dims(e, dim=1)\n",
    "\n",
    "        # [batch_size, dim, dim]\n",
    "        c_matrix = tf.matmul(v, e)\n",
    "        c_matrix_transpose = tf.transpose(c_matrix, perm=[0, 2, 1])\n",
    "\n",
    "        # [batch_size * dim, dim]\n",
    "        c_matrix = tf.reshape(c_matrix, [-1, self.dim])\n",
    "        c_matrix_transpose = tf.reshape(c_matrix_transpose, [-1, self.dim])\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        v_output = tf.reshape(tf.matmul(c_matrix, self.weight_vv) + tf.matmul(c_matrix_transpose, self.weight_ev),\n",
    "                              [-1, self.dim]) + self.bias_v\n",
    "        e_output = tf.reshape(tf.matmul(c_matrix, self.weight_ve) + tf.matmul(c_matrix_transpose, self.weight_ee),\n",
    "                              [-1, self.dim]) + self.bias_e\n",
    "\n",
    "        return v_output, e_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class MKR(object):\n",
    "    def __init__(self, args, n_users, n_items, n_entities, n_relations):\n",
    "        self._parse_args(n_users, n_items, n_entities, n_relations)\n",
    "        self._build_inputs()\n",
    "        self._build_model(args)\n",
    "        self._build_loss(args)\n",
    "        self._build_train(args)\n",
    "\n",
    "    def _parse_args(self, n_users, n_items, n_entities, n_relations):\n",
    "        self.n_user = n_users\n",
    "        self.n_item = n_items\n",
    "        self.n_entity = n_entities\n",
    "        self.n_relation = n_relations\n",
    "\n",
    "        # for computing l2 loss\n",
    "        self.vars_rs = []\n",
    "        self.vars_kge = []\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        self.user_indices = tf.placeholder(tf.int32, [None], 'user_indices')\n",
    "        self.item_indices = tf.placeholder(tf.int32, [None], 'item_indices')\n",
    "        self.labels = tf.placeholder(tf.float32, [None], 'labels')\n",
    "        self.head_indices = tf.placeholder(tf.int32, [None], 'head_indices')\n",
    "        self.tail_indices = tf.placeholder(tf.int32, [None], 'tail_indices')\n",
    "        self.relation_indices = tf.placeholder(tf.int32, [None], 'relation_indices')\n",
    "\n",
    "    def _build_model(self, args):\n",
    "        self._build_low_layers(args)\n",
    "        self._build_high_layers(args)\n",
    "\n",
    "    def _build_low_layers(self, args):\n",
    "        self.user_emb_matrix = tf.get_variable('user_emb_matrix', [self.n_user, args.dim])\n",
    "        self.item_emb_matrix = tf.get_variable('item_emb_matrix', [self.n_item, args.dim])\n",
    "        self.entity_emb_matrix = tf.get_variable('entity_emb_matrix', [self.n_entity, args.dim])\n",
    "        self.relation_emb_matrix = tf.get_variable('relation_emb_matrix', [self.n_relation, args.dim])\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)\n",
    "        self.item_embeddings = tf.nn.embedding_lookup(self.item_emb_matrix, self.item_indices)\n",
    "        self.head_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.head_indices)\n",
    "        self.relation_embeddings = tf.nn.embedding_lookup(self.relation_emb_matrix, self.relation_indices)\n",
    "        self.tail_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.tail_indices)\n",
    "\n",
    "        for _ in range(args.L):\n",
    "            user_mlp = Dense(input_dim=args.dim, output_dim=args.dim)\n",
    "            tail_mlp = Dense(input_dim=args.dim, output_dim=args.dim)\n",
    "            cc_unit = CrossCompressUnit(args.dim)\n",
    "            self.user_embeddings = user_mlp(self.user_embeddings)\n",
    "            self.item_embeddings, self.head_embeddings = cc_unit([self.item_embeddings, self.head_embeddings])\n",
    "            self.tail_embeddings = tail_mlp(self.tail_embeddings)\n",
    "\n",
    "            self.vars_rs.extend(user_mlp.vars)\n",
    "            self.vars_rs.extend(cc_unit.vars)\n",
    "            self.vars_kge.extend(tail_mlp.vars)\n",
    "            self.vars_kge.extend(cc_unit.vars)\n",
    "\n",
    "    def _build_high_layers(self, args):\n",
    "        # RS\n",
    "        use_inner_product = True\n",
    "        if use_inner_product:\n",
    "            # [batch_size]\n",
    "            self.scores = tf.reduce_sum(self.user_embeddings * self.item_embeddings, axis=1)\n",
    "        else:\n",
    "            # [batch_size, dim * 2]\n",
    "            self.user_item_concat = tf.concat([self.user_embeddings, self.item_embeddings], axis=1)\n",
    "            for _ in range(args.H - 1):\n",
    "                rs_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim * 2)\n",
    "                # [batch_size, dim * 2]\n",
    "                self.user_item_concat = rs_mlp(self.user_item_concat)\n",
    "                self.vars_rs.extend(rs_mlp.vars)\n",
    "\n",
    "            rs_pred_mlp = Dense(input_dim=args.dim * 2, output_dim=1)\n",
    "            # [batch_size]\n",
    "            self.scores = tf.squeeze(rs_pred_mlp(self.user_item_concat))\n",
    "            self.vars_rs.extend(rs_pred_mlp.vars)\n",
    "        self.scores_normalized = tf.nn.sigmoid(self.scores)\n",
    "\n",
    "        # KGE\n",
    "        # [batch_size, dim * 2]\n",
    "        self.head_relation_concat = tf.concat([self.head_embeddings, self.relation_embeddings], axis=1)\n",
    "        for _ in range(args.H - 1):\n",
    "            kge_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim * 2)\n",
    "            # [batch_size, dim]\n",
    "            self.head_relation_concat = kge_mlp(self.head_relation_concat)\n",
    "            self.vars_kge.extend(kge_mlp.vars)\n",
    "\n",
    "        kge_pred_mlp = Dense(input_dim=args.dim * 2, output_dim=args.dim)\n",
    "        # [batch_size, 1]\n",
    "        self.tail_pred = kge_pred_mlp(self.head_relation_concat)\n",
    "        self.vars_kge.extend(kge_pred_mlp.vars)\n",
    "        self.tail_pred = tf.nn.sigmoid(self.tail_pred)\n",
    "\n",
    "        self.scores_kge = tf.nn.sigmoid(tf.reduce_sum(self.tail_embeddings * self.tail_pred, axis=1))\n",
    "        self.rmse = tf.reduce_mean(\n",
    "            tf.sqrt(tf.reduce_sum(tf.square(self.tail_embeddings - self.tail_pred), axis=1) / args.dim))\n",
    "\n",
    "    def _build_loss(self, args):\n",
    "        # RS\n",
    "        self.base_loss_rs = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))\n",
    "        self.l2_loss_rs = tf.nn.l2_loss(self.user_embeddings) + tf.nn.l2_loss(self.item_embeddings)\n",
    "        for var in self.vars_rs:\n",
    "            self.l2_loss_rs += tf.nn.l2_loss(var)\n",
    "        self.loss_rs = self.base_loss_rs + self.l2_loss_rs * args.l2_weight\n",
    "\n",
    "        # KGE\n",
    "        self.base_loss_kge = -self.scores_kge\n",
    "        self.l2_loss_kge = tf.nn.l2_loss(self.head_embeddings) + tf.nn.l2_loss(self.tail_embeddings)\n",
    "        for var in self.vars_kge:\n",
    "            self.l2_loss_kge += tf.nn.l2_loss(var)\n",
    "        self.loss_kge = self.base_loss_kge + self.l2_loss_kge * args.l2_weight\n",
    "\n",
    "    def _build_train(self, args):\n",
    "        self.optimizer_rs = tf.train.AdamOptimizer(args.lr_rs).minimize(self.loss_rs)\n",
    "        self.optimizer_kge = tf.train.AdamOptimizer(args.lr_kge).minimize(self.loss_kge)\n",
    "\n",
    "    def train_rs(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer_rs, self.loss_rs], feed_dict)\n",
    "\n",
    "    def train_kge(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer_kge, self.rmse], feed_dict)\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "        return auc, acc\n",
    "\n",
    "    def get_scores(self, sess, feed_dict):\n",
    "        return sess.run([self.item_indices, self.scores_normalized], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/data_loader.py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    n_user, n_item, train_data, eval_data, test_data = load_rating(args)\n",
    "    n_entity, n_relation, kg = load_kg(args)\n",
    "    print('data loaded.')\n",
    "\n",
    "    return n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg\n",
    "\n",
    "\n",
    "def load_rating(args):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    # reading rating file\n",
    "    #rating_file = '../data/' + args.dataset + '/ratings_final'\n",
    "    rating_file = '../data/' + 'intersect-14m' + '/ratings_final'\n",
    "    if os.path.exists(rating_file + '.npy'):\n",
    "        rating_np = np.load(rating_file + '.npy')\n",
    "    else:\n",
    "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n",
    "        np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "    n_user = max(set(rating_np[:, 0])) + 1\n",
    "    n_item = max(set(rating_np[:, 1])) + 1\n",
    "    train_data, eval_data, test_data = dataset_split(rating_np)\n",
    "\n",
    "    return n_user, n_item, train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "def dataset_split(rating_np):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    # train:eval:test = 6:2:2\n",
    "    eval_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "    n_ratings = rating_np.shape[0]\n",
    "\n",
    "    eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n",
    "    left = set(range(n_ratings)) - set(eval_indices)\n",
    "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "\n",
    "    train_data = rating_np[train_indices]\n",
    "    eval_data = rating_np[eval_indices]\n",
    "    test_data = rating_np[test_indices]\n",
    "\n",
    "    return train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "def load_kg(args):\n",
    "    print('reading KG file ...')\n",
    "\n",
    "    # reading kg file\n",
    "    #kg_file = '../data/' + args.dataset + '/kg_final'\n",
    "    kg_file = '../data/' + 'intersect-14m' + '/kg_final'\n",
    "    if os.path.exists(kg_file + '.npy'):\n",
    "        kg = np.load(kg_file + '.npy')\n",
    "    else:\n",
    "        kg = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
    "        np.save(kg_file + '.npy', kg)\n",
    "\n",
    "    n_entity = max(set(kg[:, 0]) | set(kg[:, 2]))+1\n",
    "    n_relation = max(set(kg[:, 1]))+1\n",
    "\n",
    "    return n_entity, n_relation, kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataset = 'movie'\n",
    "        self.n_epoch = 20\n",
    "        self.dim = 8\n",
    "        self.L = 1\n",
    "        self.H = 1\n",
    "        self.batch_size = 4095\n",
    "        self.l2_weight = 1e-6\n",
    "        self.lr_rs = 0.000125\n",
    "        self.lr_kge = 0.000125\n",
    "        self.kge_interval = 3\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "data_info = load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = data_info[0]\n",
    "n_item = data_info[1]\n",
    "n_entity = data_info[2]\n",
    "n_relation = data_info[3]\n",
    "train_data = data_info[4]\n",
    "eval_data = data_info[5]\n",
    "test_data = data_info[6]\n",
    "kg = data_info[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 02:59:42.803033 140573074900800 deprecation.py:506] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0726 02:59:42.929726 140573074900800 deprecation.py:506] From <ipython-input-2-a1149e15351b>:47: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0726 02:59:42.933912 140573074900800 deprecation.py:506] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "W0726 02:59:42.984211 140573074900800 deprecation.py:323] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = MKR(args, n_user, n_item, n_entity, n_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"1563343228.405254\"\n",
    "CHOSEN_EPOCH = 19\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 03:00:13.978694 140573074900800 deprecation.py:323] From /home/syahbimaa/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n",
    "saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "my_set = set([1,2,3,4])\n",
    "sample = random.sample(my_set,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k =10\n",
    "sample_user = [np.random.randint(1, 15000) for i in range(0, k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1912, 9154, 6426, 4646, 13342, 8462, 8559, 8871, 4181, 11591]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
